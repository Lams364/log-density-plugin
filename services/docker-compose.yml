services:
  training:
    build:
      context: .
      dockerfile: service_model_creation/Dockerfile
    ports:
      - "8080:8080"
    volumes:
      - "training_data:/dossier_host"

  model_runner:
    build:
      context: .
      dockerfile: service_ai_analysis/Dockerfile
    ports:
      - "8081:8081"
    volumes:
      - "training_data:/dossier_host"

  ollama:
    image: ollama/ollama
    container_name: ollama
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    volumes:
      - "ollama:/root/.ollama"
    ports:
      - "11434:11434"
    command: ["serve"]

  init:
    image: ollama/ollama
    container_name: ollama-init
    depends_on:
      - ollama  # Ensure Ollama is up first
    volumes:
      - ollama:/root/.ollama
    entrypoint: ["/bin/bash", "-c"]
    command: |
      "./post-start.sh"  # Run the post-start script

volumes:
  ollama:  # Properly define the named volume here
    driver: local
  training_data:
    driver: local
    driver_opts:
      type: none
      device: "./training_data"  # location of the trainign data
      o: bind
